# copied from https://github.com/cablespaghetti/k3s-monitoring
# helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --values kube-prometheus-stack-values.yaml

# references
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

# Disable etcd monitoring. See https://github.com/cablespaghetti/k3s-monitoring/issues/4
kubeEtcd:
  enabled: false

# Need special config for controller, scheduler and proxy for k3s
# https://github.com/k3s-io/k3s/issues/3619#issuecomment-973188304
# nasbox.fritz.box = 192.168.178.10 (hacky)
kubeControllerManager:
  enabled: true
  endpoints:
  - 192.168.178.10
  service:
    enabled: true
    port: 10257
    targetPort: 10257
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
kubeScheduler:
  enabled: true
  endpoints:
  - 192.168.178.10
  service:
    enabled: true
    port: 10259
    targetPort: 10259
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
kubeProxy:
  enabled: true
  endpoints:
  - 192.168.178.10

alertmanager:
  ingress:
    enabled: false
    annotations:
      traefik.ingress.kubernetes.io/router.middlewares: prometheus-stripprefix@kubernetescrd
    hosts:
      - nasbox.fritz.box
    paths:
      - /alertmanager
    pathType: Prefix

  alertmanagerSpec:
    # see alertmanagerconfig.yml
    alertmanagerConfiguration:
      name: kube-prometheus-stack-alertmanagerconfig

    externalUrl: "http://nasbox.fritz.box/alertmanager/"
    #    replicas: 3
    #    podAntiAffinity: "soft"
    nodeSelector:
      kubernetes.io/hostname: nasbox
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    resources:
      requests:
        cpu: 25m
        memory: 32Mi

prometheus:
  ingress:
    enabled: false
    annotations:
      traefik.ingress.kubernetes.io/router.middlewares: prometheus-stripprefix@kubernetescrd
    hosts:
      - nasbox.fritz.box
    paths:
      - /prometheus
    pathType: Prefix

  prometheusSpec:
    retention: 7d
    externalUrl: "http://nasbox.fritz.box/prometheus/"

    #    replicas: 2
    #    podAntiAffinity: "hard"
    nodeSelector:
      kubernetes.io/hostname: nasbox
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    resources:
      requests:
        cpu: 200m
        memory: 1500Mi

    # scrape exporter metrics=external system, not the pods of the exporter -> no pod label etc.
    # prometheus seems to need a manual restart to pick up changes -> statefulset
    additionalScrapeConfigs:
      - job_name: modbus-exporter-kostal
        metrics_path: /modbus
        params:
          target: ["scb.fritz.box:1502"]
          module: ["kostal-inverter"]
          sub_target: ["71"] # Modbus unit identifier
        relabel_configs:
          - source_labels: [__param_target]
            target_label: instance
        static_configs:
          - targets: ["modbus-exporter.monitoring.svc.cluster.local:9602"]
      - job_name: modbus-exporter-wallbox
        metrics_path: /modbus
        params:
          target: ["go-echarger.fritz.box:502"]
          module: ["go-e-wallbox"]
          sub_target: ["1"] # Modbus unit identifier
        relabel_configs:
          - source_labels: [__param_target]
            target_label: instance
        static_configs:
          - targets: ["modbus-exporter.monitoring.svc.cluster.local:9602"]

  # every pod of the service in scraped, metrics get pod label
  additionalServiceMonitors:
  - name: prometheus-lt-kube-prometh-prometheus
    selector:
      matchLabels:
        app: kube-prometheus-stack-prometheus
        release: prometheus-lt
    endpoints:
      - path: /metrics
        port: http-web
  - name: traefik
    endpoints:
      - port: metrics
    namespaceSelector:
      matchNames:
        - kube-system
    selector:
      matchLabels:
        app: traefik
  - name: pvcontrol
    endpoints:
      - path: /metrics
        port: tcp
    selector:
      matchLabels:
        app: pvcontrol
  - name: modbus-exporter
    endpoints:
      - path: /metrics
        port: tcp
    selector:
      matchLabels:
        app: modbus-exporter
  - name: fritzbox-exporter
    endpoints:
      - path: /metrics
        port: tcp
    selector:
      matchLabels:
        app: fritzbox-exporter

additionalPrometheusRulesMap:
  pv-control.rules:
    groups:
      - name: pv-control.rules
        rules:
          - record: pvcontrol:total_charged_energy_wh_total
            expr: max(pvcontrol_controller_total_charged_energy_wh_total) by (job)
          - record: pvcontrol:charged_energy_wh_total
            expr: max(pvcontrol_controller_charged_energy_wh_total) by (job, source)
  energy-rate.rules:
    groups:
      - name: energy-rate.rules
        rules:
          - record: energy_rate_kwh_cents
            expr: vector(26.29) and vector(time()) < 1648764000 # 2022-04-01
            labels:
              type: from-grid
          - record: energy_rate_kwh_cents
            expr: vector(33.55) and vector(time()) >= 1648764000 # 2022-04-01
            labels:
              type: from-grid
          - record: energy_rate_kwh_cents
            expr: vector(7.92)
            labels:
              type: to-grid

  prometheus-node-exporter.rules:
    groups:
      - name: node-exporter-lt.rules
        rules:
          - record: instance:node_memory_MemAvailable_bytes:avg15m
            expr: avg_over_time(node_memory_MemAvailable_bytes[15m])

grafana:
  plugins:
    - grafana-piechart-panel
    - flant-statusmap-panel
  grafana.ini:
    server:
      domain: nasbox.fritz.box
      root_url: "%(protocol)s://%(domain)s/grafana"
      serve_from_sub_path: true
    auth.anonymous:
      enabled: true
      org_name: Main Org.
      org_role: Viewer
  ingress:
    enabled: false
    hosts:
      - nasbox.fritz.box
    path: /grafana

  additionalDataSources:
    # prometheus-lt instances deployed separately
    - name: prometheus-lt
      type: prometheus
      url: http://prometheus-lt-kube-prometh-prometheus:9090/
      access: proxy
      jsonData:
        timeInterval: 15m
  resources:
    requests:
      cpu: 25m
      memory: 128Mi
  sidecar:
    resources:
      requests:
        cpu: 5m
        memory: 64Mi

prometheusOperator:
  nodeSelector:
    kubernetes.io/hostname: nasbox
  resources:
    requests:
      cpu: 50m
      memory: 128Mi

prometheus-node-exporter:
  resources:
    requests:
      cpu: 10m
      memory: 16Mi

kube-state-metrics:
  nodeSelector:
    kubernetes.io/hostname: nasbox
  resources:
    requests:
      cpu: 5m
      memory: 64Mi
