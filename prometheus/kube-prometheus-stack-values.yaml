# copied from https://github.com/cablespaghetti/k3s-monitoring
# helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --values kube-prometheus-stack-values.yaml

# references
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

# Disable etcd monitoring. See https://github.com/cablespaghetti/k3s-monitoring/issues/4
kubeEtcd:
  enabled: false

# Disable kube-controller-manager and kube-scheduler monitoring. See https://github.com/cablespaghetti/k3s-monitoring/issues/2
# and https://github.com/prometheus-community/helm-charts/pull/626
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false

alertmanager:
  config:
    # global:
    #  smtp_smarthost: smtp.gmail.com:587
    #  smtp_auth_username: you@gmail.com
    #  smtp_auth_password: yourapppassword # https://support.google.com/mail/answer/185833?hl=en-GB
    #  smtp_auth_identity: you@gmail.com
    route:
      group_by: ["job"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 1h
      receiver: "null"
      routes:
        - match:
            alertname: Watchdog
          receiver: "null"
        - match:
            alertname: CPUThrottlingHigh
          receiver: "null"
        - match:
            alertname: KubeMemoryOvercommit
          receiver: "null"
        - match:
            alertname: KubeCPUOvercommit
          receiver: "null"
        - match:
            alertname: KubeletTooManyPods
          receiver: "null"

    receivers:
      - name: "null"

    # Inhibition rules allow to mute a set of alerts given that another alert is firing.
    # We use this to mute any warning-level notifications if the same alert is already critical.
    inhibit_rules:
      - source_match:
          severity: "critical"
        target_match:
          severity: "warning"
        # Apply inhibition if the alertname is the same.
        equal: ["alertname", "namespace"]

  ingress:
    enabled: true
    annotations:
      traefik.ingress.kubernetes.io/rewrite-target: /
    hosts:
      - nasbox.fritz.box
    paths:
      - /alertmanager

  alertmanagerSpec:
    externalUrl: "http://nasbox.fritz.box/alertmanager/"
    #    replicas: 3
    #    podAntiAffinity: "soft"
    nodeSelector:
      kubernetes.io/hostname: nasbox
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    resources:
      requests:
        cpu: 25m
        memory: 32Mi

prometheus:
  ingress:
    enabled: true
    annotations:
      traefik.ingress.kubernetes.io/rewrite-target: /
    hosts:
      - nasbox.fritz.box
    paths:
      - /prometheus

  prometheusSpec:
    retention: 7d
    externalUrl: "http://nasbox.fritz.box/prometheus/"

    #    replicas: 2
    #    podAntiAffinity: "hard"
    nodeSelector:
      kubernetes.io/hostname: nasbox
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    resources:
      requests:
        cpu: 200m
        memory: 400Mi

    # scrape exporter metrics=external system, not the pods of the exporter -> no pod label etc.
    # prometheus seems to need a manual restart to pick up changes -> statefulset
    additionalScrapeConfigs:
      - job_name: modbus-exporter-kostal
        metrics_path: /modbus
        params:
          target: ["scb.fritz.box:1502"]
          module: ["kostal-inverter"]
          sub_target: ["71"] # Modbus unit identifier
        relabel_configs:
          - source_labels: [__param_target]
            target_label: instance
        static_configs:
          - targets: ["modbus-exporter.monitoring.svc.cluster.local:9602"]
      - job_name: modbus-exporter-wallbox
        metrics_path: /modbus
        params:
          target: ["go-echarger.fritz.box:502"]
          module: ["go-e-wallbox"]
          sub_target: ["1"] # Modbus unit identifier
        relabel_configs:
          - source_labels: [__param_target]
            target_label: instance
        static_configs:
          - targets: ["modbus-exporter.monitoring.svc.cluster.local:9602"]

  # every pod of the service in scraped, metrics get pod label
  additionalServiceMonitors:
  - name: prometheus-lt-kube-prometh-prometheus
    selector:
      matchLabels:
        app: kube-prometheus-stack-prometheus
        release: prometheus-lt
    endpoints:
      - path: /metrics
        port: web
  - name: traefik
    endpoints:
      - port: metrics
    namespaceSelector:
      matchNames:
        - kube-system
    selector:
      matchLabels:
        app: traefik
  - name: pvcontrol
    endpoints:
      - path: /metrics
        port: tcp
    selector:
      matchLabels:
        app: pvcontrol
  - name: modbus-exporter
    endpoints:
      - path: /metrics
        port: tcp
    selector:
      matchLabels:
        app: modbus-exporter

additionalPrometheusRulesMap:
  energy-rate.rules:
    groups:
      - name: energy-rate.rules
        rules:
          - record: constant:1
            expr: 1
          - record: energy_rate_kwh_cents
            expr: label_replace(26.29*constant:1, "type", "from-grid", "","")
          - record: energy_rate_kwh_cents
            expr: label_replace(7.92*constant:1, "type", "to-grid", "","")
  prometheus-node-exporter.rules:
    groups:
      - name: node-exporter-lt.rules
        rules:
          - record: instance:node_memory_MemAvailable_bytes:avg15m
            expr: avg_over_time(node_memory_MemAvailable_bytes[15m])

grafana:
  plugins:
    - grafana-piechart-panel
    - flant-statusmap-panel
  grafana.ini:
    server:
      domain: nasbox.fritz.box
      root_url: "%(protocol)s://%(domain)s/grafana"
      serve_from_sub_path: true
    auth.anonymous:
      enabled: true
      org_name: Main Org.
      org_role: Viewer
  ingress:
    enabled: true
    hosts:
      - nasbox.fritz.box
    path: /grafana
  additionalDataSources:
    # prometheus-lt instances deployed separately
    - name: prometheus-lt
      type: prometheus
      url: http://prometheus-lt-kube-prometh-prometheus:9090/
      access: proxy
      jsonData:
        timeInterval: 15m
  resources:
    requests:
      cpu: 25m
      memory: 64Mi
  sidecar:
    resources:
      requests:
        cpu: 5m
        memory: 64Mi

prometheusOperator:
  nodeSelector:
    kubernetes.io/hostname: nasbox
  resources:
    requests:
      cpu: 50m
      memory: 128Mi

prometheus-node-exporter:
  resources:
    requests:
      cpu: 10m
      memory: 16Mi

kube-state-metrics:
  nodeSelector:
    kubernetes.io/hostname: nasbox
  resources:
    requests:
      cpu: 5m
      memory: 64Mi
