apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: prometheus
  namespace: monitoring
spec:
  # renovate: HelmChart
  repo: https://prometheus-community.github.io/helm-charts
  chart: kube-prometheus-stack
  version: 75.15.0
  targetNamespace: monitoring
  timeout: 10m
  failurePolicy: abort
  valuesContent: |-
    # copied from https://github.com/cablespaghetti/k3s-monitoring
    # helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --values kube-prometheus-stack-values.yaml

    # references
    # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

    crds:
      upgradeJob:
        enabled: true

    # etcd monitoring. See https://github.com/cablespaghetti/k3s-monitoring/issues/4
    kubeEtcd:
      enabled: true
      endpoints:
      - 192.168.178.10
      - 192.168.178.11
      - 192.168.178.12
      service:
        enabled: true
        port: 2381
        targetPort: 2381      

    # Need special config for controller, scheduler and proxy for k3s
    # https://github.com/k3s-io/k3s/issues/3619#issuecomment-973188304
    # nasbox.fritz.box = 192.168.178.10 (hacky)
    kubeControllerManager:
      enabled: true
      endpoints:
      - 192.168.178.10
      - 192.168.178.11
      - 192.168.178.12
      service:
        enabled: true
        port: 10257
        targetPort: 10257
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
    kubeScheduler:
      enabled: true
      endpoints:
      - 192.168.178.10
      - 192.168.178.11
      - 192.168.178.12
      service:
        enabled: true
        port: 10259
        targetPort: 10259
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
    kubeProxy:
      enabled: true
      endpoints:
      - 192.168.178.10
      - 192.168.178.11
      - 192.168.178.12

    alertmanager:
        # see alertmanagerconfig.yml
        alertmanagerConfiguration:
          name: kube-prometheus-stack-alertmanagerconfig

        externalUrl: "http://k3s.fritz.box/alertmanager/"
        #    replicas: 3
        #    podAntiAffinity: "soft"
        storage:
          volumeClaimTemplate:
            metadata:
              labels:
                recurring-job.longhorn.io/source: enabled
                recurring-job-group.longhorn.io/no-backup: enabled
            spec:
              accessModes: ["ReadWriteOnce"]
              storageClassName: longhorn
              resources:
                requests:
                  storage: 10Mi
        resources:
          requests:
            cpu: 25m
            memory: 32Mi

    prometheus:
      ingress:
        enabled: false
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: prometheus-stripprefix@kubernetescrd
        hosts:
          - k3s.fritz.box
        paths:
          - /prometheus
        pathType: Prefix

      prometheusSpec:
        retention: 7d
        externalUrl: "http://k3s.fritz.box/prometheus/"
        enableAdminAPI: true

        #    replicas: 2
        #    podAntiAffinity: "hard"
        # nodeSelector:
        #   kubernetes.io/hostname: nasbox
        storageSpec:
          volumeClaimTemplate:
            metadata:
              labels:
                recurring-job.longhorn.io/source: enabled
                recurring-job-group.longhorn.io/no-backup: enabled
            spec:
              accessModes: ["ReadWriteOnce"]
              storageClassName: longhorn
              resources:
                requests:
                  storage: 20Gi

        resources:
          requests:
            cpu: 200m
            memory: 2Gi

        # targets running outside of k3s
        additionalScrapeConfigs:
          - job_name: nibegw
            static_configs:
              - targets:
                - nibegw.fritz.box
                labels:

      # may want to replace/drop instance/pod labels for external systems
      additionalServiceMonitors:
      - name: prometheus-lt-kube-prometh-prometheus
        selector:
          matchLabels:
            app: kube-prometheus-stack-prometheus
            release: prometheus-lt
        endpoints:
          - path: /metrics
            port: http-web

    additionalPrometheusRulesMap:
      energy-rate.rules:
        groups:
          - name: energy-rate.rules
            rules:
              - record: energy_rate_kwh_cents
                expr: vector(26.99) and vector(time()) < 1731884400 # 2024-11-18
                labels:
                  type: from-grid
              - record: energy_rate_kwh_cents
                expr: vector(28.01) and vector(time()) >= 1731884400 # 2024-11-18
                labels:
                  type: from-grid
              - record: energy_rate_kwh_cents
                expr: vector(7.92)
                labels:
                  type: to-grid

      prometheus-node-exporter.rules:
        groups:
          - name: node-exporter-lt.rules
            rules:
              - record: instance:node_memory_MemAvailable_bytes:avg15m
                expr: avg_over_time(node_memory_MemAvailable_bytes[15m])
      
      nibe.rules:
        groups:
          - name: nibe-energy.rules
            interval: 30s
            rules:
              # nibe_pv = pv - home | truncated to [0, min(nibe,pv)]
              # car_pv = pv - home - nibe | truncated to [0, car]  # implemented in pv-control where home includes nibe
              # home=home_total - nibe - car
              #
              # nibe_pv = pv - (home_total - nibe - car) = pv - home_total + nibe + car | truncated to [0, min(nibe,pv)]
              # nibe_grid = nibe - nibe_pv
              - record: source:nibe_energy_meter_wh:rate
                expr: clamp(max(rate(kpc_home_power_consumption_wh_total{source="pv"}[5m])) - max(rate(kpc_home_power_consumption_total_wh_total[5m])) + max(rate(nibe_energy_meter_wh_total[5m])) + max(rate(pvcontrol:total_charged_energy_wh_total[5m])), 0, scalar(min(rate(nibe_energy_meter_wh_total[5m]) or rate(kpc_home_power_consumption_wh_total{source="pv"}[5m]))))
                labels:
                  source: pv
              - record: source:nibe_energy_meter_wh:rate
                expr: max(rate(nibe_energy_meter_wh_total[5m])) - max(source:nibe_energy_meter_wh:rate{source="pv"})
                labels:
                  source: grid
              - record: source:nibe_energy_meter_wh_total
                expr: (source:nibe_energy_meter_wh_total >= 0 or clamp_max(source:nibe_energy_meter_wh:rate, 0)) + source:nibe_energy_meter_wh:rate * 30
              # use temporarily for correcting values in case of trouble, delete broken time-range
              # - record: source:nibe_energy_meter_wh_total
              #   expr: "19900"
              #   labels:
              #     source: pv
              # - record: source:nibe_energy_meter_wh_total
              #   expr: "35003"
              #   labels:
              #     source: grid

    grafana:
      plugins:
        - grafana-piechart-panel
        - flant-statusmap-panel
      grafana.ini:
        server:
          domain: k3s.fritz.box
          root_url: "%(protocol)s://%(domain)s/grafana"
          serve_from_sub_path: true
        auth.anonymous:
          enabled: true
          org_name: Main Org.
          org_role: Viewer
      ingress:
        enabled: false
        hosts:
          - k3s.fritz.box
        path: /grafana

      additionalDataSources:
        # prometheus-lt instances deployed separately
        - name: prometheus-lt
          type: prometheus
          url: http://prometheus-lt-kube-prometh-prometheus:9090/
          access: proxy
          jsonData:
            timeInterval: 15m
      resources:
        requests:
          cpu: 25m
          memory: 400Mi
      sidecar:
        dashboards:
          searchNamespace: ALL
        resources:
          requests:
            cpu: 5m
            memory: 64Mi
      # fix for https://github.com/prometheus-community/helm-charts/issues/1867 - missing release label on grafana service monitor
      serviceMonitor:
        labels:
          release: prometheus

    prometheusOperator:
      resources:
        requests:
          cpu: 50m
          memory: 128Mi

    prometheus-node-exporter:
      resources:
        requests:
          cpu: 10m
          memory: 16Mi

    kube-state-metrics:
      resources:
        requests:
          cpu: 5m
          memory: 64Mi
